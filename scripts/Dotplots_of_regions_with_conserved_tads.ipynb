{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conserved TAD example 1 region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chromosome .fna files created in the 'NPB' folder.\n"
     ]
    }
   ],
   "source": [
    "npb chr01:2500000-2900000\n",
    "oruf chr01:2200000-2600000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting region sequences from genomes: npb_exampleX.fna, oruf_exampleX.fna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sequence_from_genomic_coordinates(fasta_file, chromosome, start, end, output_file):\n",
    "    sequence = ''\n",
    "    with open(fasta_file, 'r') as infile:\n",
    "        current_header = ''\n",
    "        for line in infile:\n",
    "            if line.startswith('>'):\n",
    "                current_header = line.strip()\n",
    "            elif current_header == f'>{chromosome}':\n",
    "                sequence += line.strip()\n",
    "\n",
    "    start_index = start - 1  # Adjusting to 0-based indexing\n",
    "    end_index = end  # No need to adjust the end index\n",
    "\n",
    "    extracted_sequence = sequence[start_index:end_index]\n",
    "\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        outfile.write(f\">{chromosome}:{start}-{end}\\n{extracted_sequence}\\n\")\n",
    "\n",
    "# Define input and output files\n",
    "fasta_file = '/scratch/ak8725/genomes/orufi.fna'\n",
    "chromosome = 'chr01'\n",
    "start_coordinate = 2200000\n",
    "end_coordinate = 2600000\n",
    "output_file = '/scratch/ak8725/dotplots/oruf_example1.fna'\n",
    "\n",
    "# Extract the sequence for the specified region\n",
    "extract_sequence_from_genomic_coordinates(fasta_file, chromosome, start_coordinate, end_coordinate, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Running mummer, gnuplot and converting .ps into .png format with combined output. Visualization of collinearity with dotplots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sbatch dotplot_region.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Identifying SNPs, SV coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Did not do this, all analysis done by MUM&Co\n",
    "#filter delta file (optional)\n",
    "#the inv coordinates were the same with filtered and non-filtered delta files\n",
    "#I used unfiltered for the rest of rearrangements\n",
    "#delta-filter -m -i 90 -l 100 ./NPB-az_alignments/chr06.delta > ./NPB-az_alignments/chr06_filtered.delta\n",
    "#convert delta file into .coords file\n",
    "# module load mummer/intel/4.0.0rc1\n",
    "# show-coords -Trd example1.delta > example1.coords\n",
    "# look into .coords file and identify blocks with -1 in the FRM column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nucmer alignment of genomes, filtering and converting to coordinates\n",
      "\n",
      "                               #\n",
      "                              # #\n",
      "                             #   #\n",
      "                ###############################\n",
      "                #                             #\n",
      "                # MUM&Co is open for business #\n",
      "                #           version 3.8       #\n",
      "                ###############################\n",
      "\n",
      "\n",
      "######################################################################################################\n",
      "          USING GLOBAL ALIGNMENT COORDINATES FOR DELETIONS, INSERTIONS AND TRANSLOCATIONS\n",
      "######################################################################################################\n",
      "\n",
      "\n",
      "Matching query and reference chromosomes\n",
      "\n",
      "98.1264\n",
      "\n",
      "Finding alignment gaps\n",
      "\n",
      "\n",
      "Filtering for size labelling SV\n",
      "\n",
      "\n",
      "Finding translocation fragments\n",
      "\n",
      "\n",
      "Checking alignment sense for inversions involving majority of single chromosome bases\n",
      "\n",
      "\n",
      "\n",
      "######################################################################################################\n",
      "             USING NON-GLOBAL ALIGNMENT FOR INVERSIONS, DUPLICATIONS AND CONTRACTIONS\n",
      "######################################################################################################\n",
      "\n",
      "\n",
      "Matching chromosomes based on names, using '_' seperator and filtering chrMT\n",
      "\n",
      "\n",
      "Looking for interchromosomal changes in alignment sense, filtering for 1kb fragments and merging closely neighbouring calls\n",
      "\n",
      "\n",
      "Locating alignment overlaps for duplication assignment\n",
      "\n",
      "\n",
      "Locating alignment overlaps for contraction assignment\n",
      "\n",
      "\n",
      "\n",
      "##############################################################################################################\n",
      "Combining both deletions, insertion, inversions and duplications and identifying regions of more than one call\n",
      "##############################################################################################################\n",
      "\n",
      "\n",
      "Filtering clean inversions using deletion and insertion information\n",
      "\n",
      "\n",
      "Combining all called SVs\n",
      "\n",
      "\n",
      "Removing deletions and insertions called during global alignment due to inversions\n",
      "\n",
      "\n",
      "Extracting DNA involved in SV events\n",
      "\n",
      "\n",
      "Adding translocation border information to last column\n",
      "\n",
      "\n",
      "Generating a VCF file\n",
      "\n",
      "\n",
      "Counting number of detected SVs\n",
      "\n",
      "\n",
      "example1  Total SVs  = 51\n",
      "example1  Deletions  = 21\n",
      "example1  Insertions  = 26\n",
      "example1  Duplications  = 4\n",
      "example1  Contractions  = 0\n",
      "example1  Inversions  = 0\n",
      "example1  Translocations  = 0\n"
     ]
    }
   ],
   "source": [
    "#detect SVs with mum&co \n",
    "#https://github.com/SAMtoBAM/MUMandCo\n",
    "module purge\n",
    "module load samtools/intel/1.14\n",
    "module load mummer/intel/4.0.0rc1\n",
    "bash mumandco_v3.8.sh -r npb_example1.fna -q oruf_example1.fna -g 400000 -o example1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract query (oruf) coordinates of SVs into bed file, creating 1_svs_query.bed\n",
    "#it is to be plotted in coolbox with the query (oruf)\n",
    "def create_bed_file(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        # Skip the header\n",
    "        next(infile)\n",
    "        # Iterate over each line in the input file\n",
    "        for line in infile:\n",
    "            # Split the line into columns\n",
    "            columns = line.strip().split('\\t')\n",
    "            # Check if the line has enough columns\n",
    "            if len(columns) >= 8:\n",
    "                # Extract required information\n",
    "                start = int(columns[6]) + 2200000 - 1  # Extract query_start\n",
    "                end = int(columns[7]) + 2200000 - 1  # Extract query_stop\n",
    "                sv_type = columns[5]  # Extract SV_type\n",
    "                # Write to the output file in BED format\n",
    "                outfile.write(f\"chr01\\t{start}\\t{end}\\t{sv_type}\\t1\\t+\\n\")\n",
    "            else:\n",
    "                print(f\"Issue parsing line: {line}\")\n",
    "\n",
    "# Define input and output file names\n",
    "input_file = './example1_output/example1.SVs_all.tsv'\n",
    "output_file = '1_svs_query.bed'\n",
    "\n",
    "# Create the BED file\n",
    "create_bed_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the file to make it compatible for coolbox\n",
    "# Define the file path\n",
    "file_path = '1_svs_query.bed'\n",
    "\n",
    "# Read the file, process the lines, and write the modified lines back to the file\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Process each line\n",
    "modified_lines = []\n",
    "for line in lines:\n",
    "    # Split the line into columns\n",
    "    columns = line.split('\\t')\n",
    "    # Check if the value in col3 is equal to the value in col2\n",
    "    if int(columns[2]) == int(columns[1]):\n",
    "        # Increment the value in col3 by 1\n",
    "        columns[2] = str(int(columns[2]) + 1)\n",
    "    # Join the columns back into a line\n",
    "    modified_line = '\\t'.join(columns)\n",
    "    modified_lines.append(modified_line)\n",
    "\n",
    "# Write the modified lines back to the file\n",
    "with open(file_path, 'w') as file:\n",
    "    file.writelines(modified_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create separate file for ins,del,dup\n",
    "# Define the input file path\n",
    "input_file = '1_svs_query.bed'\n",
    "\n",
    "# Define output file paths\n",
    "output_files = {\n",
    "    'duplication': '1_dup.bed',\n",
    "    'insertion': '1_ins.bed',\n",
    "    'deletion': '1_del.bed'\n",
    "}\n",
    "\n",
    "# Open the input file and read lines\n",
    "with open(input_file, 'r') as infile:\n",
    "    # Iterate through each line in the input file\n",
    "    for line in infile:\n",
    "        # Split the line into columns\n",
    "        columns = line.strip().split('\\t')\n",
    "        # Extract the SV type from the fourth column\n",
    "        sv_type = columns[3]\n",
    "        # Write the line to the appropriate output file based on the SV type\n",
    "        with open(output_files.get(sv_type), 'a') as outfile:\n",
    "            outfile.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
