{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Analyzing TAD conservation with BLAST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy 4\n",
    "## Detecting conserved TADs with SVs\n",
    "## Detecting split/merge events\n",
    "### Finding % identity of NPB TADs in Oruf\n",
    "1. Blasting all NPB TADs to oruf TADs with no min perc_ident or alignment rate, no max_target_seqs\n",
    "2. Extracting all hits per TAD (1 TAD - 1 target), summarizing, recording query coverage\n",
    "3. Identifying average/median query_cov, plotting their distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index file ../genomes/azucena.fna.fai not found, generating...\n",
      "index file ../genomes/IR64.fna.fai not found, generating...\n",
      "index file ../genomes/orufi.fna.fai not found, generating...\n",
      "index file ../genomes/omer.fna.fai not found, generating...\n"
     ]
    }
   ],
   "source": [
    "#1. Extracting TAD sequences to x_tads.fna\n",
    "module load bedtools/intel/2.29.2\n",
    "bedtools getfasta -fi ../genomes/NPB.fna -bed ../comparative_TADs_boundaries/NPB_TADs_ranked.bed -fo npb_tads.fna\n",
    "bedtools getfasta -fi ../genomes/azucena.fna -bed ../comparative_TADs_boundaries/az_TADs_5kb_two_tools_80.bed -fo az_tads.fna\n",
    "bedtools getfasta -fi ../genomes/IR64.fna -bed ../comparative_TADs_boundaries/IR64_TADs_5kb_two_tools_80.bed -fo ir64_tads.fna\n",
    "bedtools getfasta -fi ../genomes/orufi.fna -bed ../comparative_TADs_boundaries/oruf_TADs_5kb_two_tools_80.bed -fo oruf_tads.fna\n",
    "bedtools getfasta -fi ../genomes/omer.fna -bed ../comparative_TADs_boundaries/omer_TADs_5kb_two_tools_80.bed -fo omer_tads.fna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 40677642\n"
     ]
    }
   ],
   "source": [
    "#2. npb_oruf_all_blast_results4.txt\n",
    "#query = npb\n",
    "#targets = az,ir64,oruf,omer\n",
    "cd ../Method_4/\n",
    "sbatch blast.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the results so that only TADs on the same chr in query and target remain, and the length of hsp alignment is minimum 5000\n",
    "import pandas as pd\n",
    "\n",
    "# Read the input file\n",
    "file_path = 'npb_omer_all_blast_results4.txt'\n",
    "df = pd.read_table(file_path, header=None, names=['col1', 'col2', 'identity', 'length', 'mismatches', 'gap_openings', 'q_start', 'q_end', 's_start', 's_end', 'e_value', 'bit_score', 'alignment_length'])\n",
    "\n",
    "# Extract numbers after 'chr' in col1 and col2\n",
    "df['chr1_number'] = df['col1'].str.extract(r'chr(\\d+)')\n",
    "df['chr2_number'] = df['col2'].str.extract(r'chr(\\d+)')\n",
    "\n",
    "# Filter rows based on conditions\n",
    "filtered_df = df[(df['chr1_number'] == df['chr2_number']) & (df['length'] >= 5000)]\n",
    "\n",
    "# Drop the temporary columns\n",
    "filtered_df = filtered_df.drop(['chr1_number', 'chr2_number'], axis=1)\n",
    "\n",
    "# Save the filtered DataFrame to a new file\n",
    "filtered_file_path = 'npb_omer_all_blast_results4_filtered.txt'\n",
    "filtered_df.to_csv(filtered_file_path, sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Per NPB_TAD, report all oruf_TADs and their cumulative coverages\n",
    "# output file structure: npb_TAD identifier, \n",
    "#                         oruf_TAD-n identifier\n",
    "#                         Qlen (col13) TAD length in NPB\n",
    "#                         cumulative Tlength (oruf_TAD-n length) = sum of col4 values per entry in col1+col2 of input file\n",
    "#                         percentage Tlen/Qlen\n",
    "                        \n",
    "import pandas as pd\n",
    "\n",
    "# Read the input file\n",
    "file_path = 'npb_omer_all_blast_results4_filtered.txt'\n",
    "df = pd.read_table(file_path, header=None, names=['col1', 'col2', 'identity', 'length', 'mismatches', 'gap_openings', 'q_start', 'q_end', 's_start', 's_end', 'e_value', 'bit_score', 'alignment_length'])\n",
    "\n",
    "# Create a column for sorting based on the original order\n",
    "df['sort_order'] = range(len(df))\n",
    "\n",
    "# Group by col1 and col2, and calculate the sum of col4 for each group\n",
    "grouped_df = df.groupby(['col1', 'col2'], as_index=False)['length'].sum()\n",
    "\n",
    "# Merge with the original DataFrame to get the last col value\n",
    "summary_df = pd.merge(df[['col1', 'col2', 'alignment_length', 'sort_order']], grouped_df, on=['col1', 'col2'])\n",
    "\n",
    "# Select the first row of each group\n",
    "summary_df = summary_df.groupby(['col1', 'col2']).first().reset_index()\n",
    "\n",
    "# Sort the summary DataFrame based on the original order\n",
    "summary_df = summary_df.sort_values(by='sort_order').drop('sort_order', axis=1)\n",
    "\n",
    "# Calculate col5\n",
    "summary_df['col5'] = (summary_df['length'] / summary_df['alignment_length']) * 100\n",
    "\n",
    "# Save the summary DataFrame to a new file\n",
    "summary_file_path = 'npb_omer_summary4.txt'\n",
    "summary_df.to_csv(summary_file_path, sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each NPB_TAD, I will check all omer_TADs \n",
    "#If the omer_TAD-2 start is farther than 100kb from omer_TAD1 end, it will be discarded\n",
    "#continue until all oruf_TADs processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr01:420000-525000: ['chr01:575000-655000']\n",
      "chr01:525000-560000: ['chr01:655000-695000']\n",
      "chr01:2590000-2640000: ['chr01:2295000-2350000']\n",
      "chr01:2640000-2695000: ['chr01:2350000-2395000']\n",
      "chr01:2890000-2965000: ['chr01:8485000-8695000']\n"
     ]
    }
   ],
   "source": [
    "#create a library (key = npb_tad, value = list of oruf_tads)\n",
    "def create_dict_from_file(file_path):\n",
    "    result_dict = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            key = parts[0]\n",
    "            value = parts[1]\n",
    "\n",
    "            if key not in result_dict:\n",
    "                result_dict[key] = [value]\n",
    "            else:\n",
    "                result_dict[key].append(value)\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "# Example usage\n",
    "file_path = 'npb_omer_summary4.txt'\n",
    "result_dict = create_dict_from_file(file_path)\n",
    "\n",
    "# Print the first three key-value combinations\n",
    "count = 0\n",
    "for key, values in result_dict.items():\n",
    "    print(f\"{key}: {values}\")\n",
    "    count += 1\n",
    "    if count == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function to filter oruf_tads for each npb_tad, so that the oruf_tads are not farther than 100kb from each other\n",
    "def filter_values(dictionary):\n",
    "    filtered_dict = {}\n",
    "\n",
    "    for key, values in dictionary.items():\n",
    "        if len(values) == 1:\n",
    "            # If there is only one value, keep it as is\n",
    "            filtered_dict[key] = values\n",
    "        else:\n",
    "            # If there are multiple values, process them\n",
    "            filtered_values = [values[0]]  # Keep the first value\n",
    "    \n",
    "            for i in range(1, len(values)):\n",
    "                # Split each value by ':' and '-'\n",
    "                parts1 = values[i - 1].split(':')[-1].split('-')\n",
    "                parts1.insert(0, values[i - 1].split(':')[0])\n",
    "\n",
    "                parts2 = values[i].split(':')[-1].split('-')\n",
    "                parts2.insert(0, values[i].split(':')[0])\n",
    "                \n",
    "                # Compare the parts and apply the filtering conditions\n",
    "                if parts1[0] == parts2[0] and abs(int(parts2[1]) - int(parts1[2])) < 100000:\n",
    "                    filtered_values.append(values[i])\n",
    "                    \n",
    "            filtered_dict[key] = filtered_values\n",
    "\n",
    "    return filtered_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr01:420000-525000: ['chr01:575000-655000']\n",
      "chr01:525000-560000: ['chr01:655000-695000']\n",
      "chr01:2590000-2640000: ['chr01:2295000-2350000']\n",
      "chr01:2640000-2695000: ['chr01:2350000-2395000']\n",
      "chr01:2890000-2965000: ['chr01:8485000-8695000']\n"
     ]
    }
   ],
   "source": [
    "# Assuming `result_dict` is the initial dictionary\n",
    "filtered_dict = filter_values(result_dict)\n",
    "\n",
    "# Print the first three key-value combinations\n",
    "count = 0\n",
    "for key, values in filtered_dict.items():\n",
    "    print(f\"{key}: {values}\")\n",
    "    count += 1\n",
    "    if count == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a npb_oruf_summary4_filtered.txt from filtered_dict\n",
    "#for each npb_tad, I have oruf_tads that it is split into\n",
    "\n",
    "import csv\n",
    "\n",
    "# Load the contents of npb_oruf_summary4.txt into a list of lines\n",
    "with open('npb_omer_summary4.txt', 'r') as file:\n",
    "    summary_lines = file.readlines()\n",
    "\n",
    "# Create a list to store the filtered lines\n",
    "filtered_lines = []\n",
    "\n",
    "# Iterate through the entries in the filtered_dict\n",
    "for key, values in filtered_dict.items():\n",
    "    # Iterate through the values for each key\n",
    "    for value in values:\n",
    "        # Check if the entry (key + '\\t' + value) is present in npb_oruf_summary4.txt\n",
    "        entry = f\"{key}\\t{value}\"\n",
    "        for line in summary_lines:\n",
    "            if entry + '\\t' in line:\n",
    "                # If the entry is found, add the entire row to the filtered lines list\n",
    "                filtered_lines.append(line.strip() + '\\n')  # Add a newline character\n",
    "                break  # Stop searching for the same entry in subsequent lines\n",
    "\n",
    "# Write the filtered lines to npb_oruf_summary4_filtered.txt\n",
    "with open('npb_omer_summary4_filtered.txt', 'w', newline='') as file:\n",
    "    file.writelines(filtered_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate number of conserved TADs from npb_oruf_summary4_filtered.txt\n",
    "#conserved would be rows with unique entries in both col1 and col2\n",
    "import csv\n",
    "\n",
    "def save_conserved_rows(input_file, output_file):\n",
    "    col1_count = {}\n",
    "    col2_count = {}\n",
    "    conserved_rows = []\n",
    "\n",
    "    with open(input_file, 'r', newline='') as file:\n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        header = next(reader, None)  # Read the header if present\n",
    "        if header:\n",
    "            conserved_rows.append(header)\n",
    "\n",
    "        for row in reader:\n",
    "            col1_value = row[0]\n",
    "            col2_value = row[1]\n",
    "\n",
    "            # Check uniqueness in col1\n",
    "            if col1_value not in col1_count:\n",
    "                col1_count[col1_value] = 1\n",
    "            else:\n",
    "                col1_count[col1_value] += 1\n",
    "\n",
    "            # Check uniqueness in col2\n",
    "            if col2_value not in col2_count:\n",
    "                col2_count[col2_value] = 1\n",
    "            else:\n",
    "                col2_count[col2_value] += 1\n",
    "\n",
    "            # If both col1 and col2 are unique, add the row to conserved_rows\n",
    "            if col1_count[col1_value] == 1 and col2_count[col2_value] == 1:\n",
    "                conserved_rows.append(row)\n",
    "\n",
    "    # Write the conserved rows to the output file\n",
    "    with open(output_file, 'w', newline='') as output:\n",
    "        writer = csv.writer(output, delimiter='\\t')\n",
    "        writer.writerows(conserved_rows)\n",
    "\n",
    "# Example usage\n",
    "input_file = 'npb_omer_summary4_filtered.txt'\n",
    "output_file = 'npb_omer_conserved.txt'\n",
    "save_conserved_rows(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean coverage: 19.30\n",
      "Median coverage: 14.11\n",
      "Total conserved TADs: 418\n",
      "Coverage >= 50: 25 TADs\n",
      "Coverage >= 80: 1 TADs\n",
      "Coverage >= 90: 0 TADs\n",
      "Coverage >= 100: 0 TADs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### How many TADs are conserved NPB-Oruf\n",
    "# Read the data from the file\n",
    "data = pd.read_csv(\"npb_omer_conserved.txt\", sep='\\t', header=None, names=['npb_tad', 'az_tad', 'npb_len', 'az_len', 'coverage'])\n",
    "\n",
    "    # Calculate and print mean coverage\n",
    "mean_coverage = data['coverage'].mean()\n",
    "print(f\"Mean coverage: {mean_coverage:.2f}\")\n",
    "\n",
    "    # Calculate and print median coverage\n",
    "median_coverage = data['coverage'].median()\n",
    "print(f\"Median coverage: {median_coverage:.2f}\")\n",
    "\n",
    "    # Calculate and print total number of rows\n",
    "total_rows = len(data)\n",
    "print(f\"Total conserved TADs: {total_rows}\")\n",
    "\n",
    "    # Calculate and print proportion of rows with coverage >= 50, 80, 90, 100\n",
    "coverage_above_50 = data[data['coverage'] >= 50]\n",
    "coverage_above_80 = data[data['coverage'] >= 80]\n",
    "coverage_above_90 = data[data['coverage'] >= 90]\n",
    "coverage_above_100 = data[data['coverage'] >= 100]\n",
    "    \n",
    "    # Calculate and print number of rows with coverage >= 50, 80, 90, 100\n",
    "print(f\"Coverage >= 50: {len(coverage_above_50)} TADs\")\n",
    "print(f\"Coverage >= 80: {len(coverage_above_80)} TADs\")\n",
    "print(f\"Coverage >= 90: {len(coverage_above_90)} TADs\")\n",
    "print(f\"Coverage >= 100: {len(coverage_above_100)} TADs\\n\")\n",
    "\n",
    "#compare with results from Method3\n",
    "# Coverage >= 50: 555 TADs\n",
    "# Coverage >= 80: 486 TADs\n",
    "# Coverage >= 90: 438 TADs\n",
    "# Coverage >= 100: 305 TADs\n",
    "#the difference is due to filtering: length of hsp alignment is minimum 5000 in Method4, any in Method3\n",
    "#I'm gonna use results of Method3 for conserved TADs annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of split events: 11\n"
     ]
    }
   ],
   "source": [
    "#continue here - is the calculation of split tads correct?\n",
    "#detecting split events and saving to npb_oruf_splits.txt\n",
    "#col1=1, col2>=2, sum(col4)>=50\n",
    "\n",
    "import csv\n",
    "\n",
    "def detect_and_save_events(input_file, output_file):\n",
    "    # Load the input file into a list of lists\n",
    "    with open(input_file, 'r', newline='') as file:\n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        data = list(reader)\n",
    "\n",
    "    # Create a dictionary to store the count of unique strings from col1\n",
    "    col1_count = {}\n",
    "\n",
    "    # Create a list to store rows that satisfy the conditions\n",
    "    filtered_rows = []\n",
    "\n",
    "    # Iterate through the rows in the data\n",
    "    for row in data:\n",
    "        col1_value = row[0]  # Assuming col1 is at index 0\n",
    "        col2_value = row[1]  # Assuming col2 is at index 1\n",
    "        col4_value = int(row[3])  # Assuming col4 is at index 3\n",
    "\n",
    "        # Update col1_count dictionary\n",
    "        if col1_value not in col1_count:\n",
    "            col1_count[col1_value] = {col2_value}\n",
    "        else:\n",
    "            col1_count[col1_value].add(col2_value)\n",
    "\n",
    "        # Check conditions for detecting events\n",
    "        if len(col1_count[col1_value]) >= 2 and sum(int(row[3]) for row in data if row[0] == col1_value) >= 0:\n",
    "            filtered_rows.extend([row for row in data if row[0] == col1_value])\n",
    "\n",
    "    # Write the filtered rows to the output file\n",
    "    with open(output_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter='\\t')\n",
    "        writer.writerows(filtered_rows)\n",
    "\n",
    "    # Return the number of events\n",
    "    num_events = sum(1 for col1_value, count in col1_count.items() if len(count) >= 2 and sum(int(row[3]) for row in data if row[0] == col1_value) >= 50)\n",
    "    return num_events\n",
    "\n",
    "# Example usage\n",
    "input_file = 'npb_omer_summary4_filtered.txt'\n",
    "output_file = 'npb_omer_splits.txt'\n",
    "num_events = detect_and_save_events(input_file, output_file)\n",
    "\n",
    "print(f\"Number of split events: {num_events}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of merge events (non-filtered for distance): 89\n"
     ]
    }
   ],
   "source": [
    "#detecting merge events and saving to npb_oruf_merges.txt\n",
    "#col1>=2, col2=1, sum(col4)>=50\n",
    "\n",
    "import csv\n",
    "\n",
    "def detect_and_save_events(input_file, output_file):\n",
    "    # Load the input file into a list of lists\n",
    "    with open(input_file, 'r', newline='') as file:\n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        data = list(reader)\n",
    "\n",
    "    # Create a dictionary to store the count of unique strings from col2\n",
    "    col2_count = {}\n",
    "\n",
    "    # Create a list to store rows that satisfy the conditions\n",
    "    filtered_rows = []\n",
    "\n",
    "    # Iterate through the rows in the data\n",
    "    for row in data:\n",
    "        col1_value = row[0]  # Assuming col1 is at index 0\n",
    "        col2_value = row[1]  # Assuming col2 is at index 1\n",
    "        col4_value = int(row[3])  # Assuming col4 is at index 3\n",
    "\n",
    "        # Update col2_count dictionary\n",
    "        if col2_value not in col2_count:\n",
    "            col2_count[col2_value] = {col1_value}\n",
    "        else:\n",
    "            col2_count[col2_value].add(col1_value)\n",
    "\n",
    "        # Check conditions for detecting events\n",
    "        if len(col2_count[col2_value]) >= 2 and sum(int(row[3]) for row in data if row[1] == col2_value) >= 0:\n",
    "            filtered_rows.extend([row for row in data if row[1] == col2_value])\n",
    "\n",
    "    # Write the filtered rows to the output file\n",
    "    with open(output_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter='\\t')\n",
    "        writer.writerows(filtered_rows)\n",
    "\n",
    "    # Return the number of events\n",
    "    num_events = sum(1 for col2_value, count in col2_count.items() if len(count) >= 2 and sum(int(row[3]) for row in data if row[1] == col2_value) >= 50)\n",
    "    return num_events\n",
    "\n",
    "# Example usage\n",
    "input_file = 'npb_omer_summary4_filtered.txt'\n",
    "output_file = 'npb_omer_merges_draft.txt'\n",
    "num_events = detect_and_save_events(input_file, output_file)\n",
    "\n",
    "print(f\"Number of merge events (non-filtered for distance): {num_events}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr01:1880000-1940000: ['chr01:14835000-14995000', 'chr01:15285000-15385000', 'chr01:22260000-22335000', 'chr01:23435000-23515000']\n",
      "chr01:22010000-22110000: ['chr01:22080000-22157500', 'chr01:22157500-22260000']\n",
      "chr01:22205000-22290000: ['chr01:22335000-22385000', 'chr01:22385000-22450000']\n"
     ]
    }
   ],
   "source": [
    "#filtering the merge events from npb_oruf_merges_draft.txt so that the distance between npb_tads is <=100kb\n",
    "#create a library (key = oruf_tad, value = list of npb_tads)\n",
    "def create_dict_from_file(file_path):\n",
    "    result_dict = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            key = parts[1]\n",
    "            value = parts[0]\n",
    "\n",
    "            if key not in result_dict:\n",
    "                result_dict[key] = [value]\n",
    "            else:\n",
    "                # Add the value only if it's not already in the list\n",
    "                if value not in result_dict[key]:\n",
    "                    result_dict[key].append(value)\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "# Example usage\n",
    "file_path = 'npb_omer_merges_draft.txt'\n",
    "result_dict = create_dict_from_file(file_path)\n",
    "\n",
    "# Print the first three key-value combinations\n",
    "count = 0\n",
    "for key, values in result_dict.items():\n",
    "    print(f\"{key}: {values}\")\n",
    "    count += 1\n",
    "    if count == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr01:1880000-1940000: ['chr01:14835000-14995000']\n",
      "chr01:22010000-22110000: ['chr01:22080000-22157500', 'chr01:22157500-22260000']\n",
      "chr01:22205000-22290000: ['chr01:22335000-22385000', 'chr01:22385000-22450000']\n",
      "chr01:25360000-25420000: ['chr01:25510000-25585000', 'chr01:25585000-25632500']\n",
      "chr01:21905000-22010000: ['chr01:21975000-22080000']\n"
     ]
    }
   ],
   "source": [
    "#create function to filter npb_tads for each oruf_tad, so that the npb_tads are not farther than 100kb from each other\n",
    "def filter_values(dictionary):\n",
    "    filtered_dict = {}\n",
    "\n",
    "    for key, values in dictionary.items():\n",
    "        if len(values) == 1:\n",
    "            # If there is only one value, keep it as is\n",
    "            filtered_dict[key] = values\n",
    "        else:\n",
    "            # If there are multiple values, process them\n",
    "            filtered_values = [values[0]]  # Keep the first value\n",
    "    \n",
    "            for i in range(1, len(values)):\n",
    "                # Split each value by ':' and '-'\n",
    "                parts1 = values[i - 1].split(':')[-1].split('-')\n",
    "                parts1.insert(0, values[i - 1].split(':')[0])\n",
    "\n",
    "                parts2 = values[i].split(':')[-1].split('-')\n",
    "                parts2.insert(0, values[i].split(':')[0])\n",
    "                \n",
    "                # Compare the parts and apply the filtering conditions\n",
    "                if parts1[0] == parts2[0] and abs(int(parts2[1]) - int(parts1[2])) < 100000:\n",
    "                    filtered_values.append(values[i])\n",
    "                    \n",
    "            filtered_dict[key] = filtered_values\n",
    "\n",
    "    return filtered_dict\n",
    "\n",
    "# Assuming `result_dict` is the initial dictionary\n",
    "filtered_dict = filter_values(result_dict)\n",
    "\n",
    "# Print the first three key-value combinations\n",
    "count = 0\n",
    "for key, values in filtered_dict.items():\n",
    "    print(f\"{key}: {values}\")\n",
    "    count += 1\n",
    "    if count == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a npb_oruf_merges.txt from filtered_dict\n",
    "#for each oruf_tad, I have npb_tads that it is split into\n",
    "#order of cols: npb_tad, oruf_tad, etc\n",
    "\n",
    "import csv\n",
    "\n",
    "# Load the contents of npb_oruf_merges_draft.txt into a list of lines\n",
    "with open('npb_omer_merges_draft.txt', 'r') as file:\n",
    "    summary_lines = file.readlines()\n",
    "\n",
    "# Create a list to store the filtered lines\n",
    "filtered_lines = []\n",
    "\n",
    "# Iterate through the entries in the filtered_dict\n",
    "for key, values in filtered_dict.items():\n",
    "    # Iterate through the values for each key\n",
    "    for value in values:\n",
    "        # Check if the entry (key + '\\t' + value) is present in npb_oruf_merges_draft.txt\n",
    "        entry = f\"{value}\\t{key}\"\n",
    "        for line in summary_lines:\n",
    "            if entry + '\\t' in line:\n",
    "                # If the entry is found, add the entire row to the filtered lines list\n",
    "                filtered_lines.append(line.strip() + '\\n')  # Add a newline character\n",
    "                break  # Stop searching for the same entry in subsequent lines\n",
    "\n",
    "# Write the filtered lines to npb_oruf_summary4_filtered.txt\n",
    "with open('npb_omer_merges.txt', 'w', newline='') as file:\n",
    "    file.writelines(filtered_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create file npb_omer_merges_true.txt with real merge events\n",
    "import csv\n",
    "\n",
    "def write_filtered_entries(input_file, output_file):\n",
    "    col2_entries = {}\n",
    "\n",
    "    with open(input_file, 'r', newline='') as file:\n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        next(reader)  # Skip header if present\n",
    "        for row in reader:\n",
    "            col1_value = row[0]  # Assuming col1 is at index 0\n",
    "            col2_value = row[1]  # Assuming col2 is at index 1\n",
    "\n",
    "            if col2_value not in col2_entries:\n",
    "                col2_entries[col2_value] = {col1_value}\n",
    "            else:\n",
    "                col2_entries[col2_value].add(col1_value)\n",
    "\n",
    "    # Write unique entries in col2 with two or more unique entries in col1 to the output file\n",
    "    with open(output_file, 'w', newline='') as outfile:\n",
    "        writer = csv.writer(outfile, delimiter='\\t')\n",
    "        writer.writerow([\"Col1\", \"Col2\"])  # Write header\n",
    "\n",
    "        for col2_value, col1_set in col2_entries.items():\n",
    "            if len(col1_set) >= 2:\n",
    "                for col1_value in col1_set:\n",
    "                    writer.writerow([col1_value, col2_value])\n",
    "\n",
    "input_file = 'npb_omer_merges.txt'\n",
    "output_file = 'npb_omer_merges_true.txt'\n",
    "write_filtered_entries(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of merge events: 30\n"
     ]
    }
   ],
   "source": [
    "#continue here - is the calculation of merge tads correct?\n",
    "#calculate number of merge events (2+ npb_tads into 1 oruf_tad) filtered for distance (<=100kb)\n",
    "import csv\n",
    "\n",
    "def count_events(input_file):\n",
    "    col2_count = {}\n",
    "\n",
    "    with open(input_file, 'r', newline='') as file:\n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        next(reader)  # Skip header if present\n",
    "        for row in reader:\n",
    "            col1_value = row[0]  # Assuming col1 is at index 0\n",
    "            col2_value = row[1]  # Assuming col2 is at index 1\n",
    "\n",
    "            if col2_value not in col2_count:\n",
    "                col2_count[col2_value] = {col1_value}\n",
    "            else:\n",
    "                col2_count[col2_value].add(col1_value)\n",
    "\n",
    "    # Count the number of unique entries in col2 with two or more unique entries in col1\n",
    "    num_events = sum(1 for count in col2_count.values() if len(count) >= 2)\n",
    "\n",
    "    return num_events\n",
    "\n",
    "input_file = 'npb_omer_merges.txt'\n",
    "num_events = count_events(input_file)\n",
    "\n",
    "print(f\"Number of merge events: {num_events}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nipponbare-Oruf comparison\n",
    "1. How many split events happened? (1 NPB TAD split into 2/3/n Oruf TADs) 11\n",
    "2. How many merge events happened? (2/3/n NPB TADs merged into 1 Oruf TAD) 30\n",
    "3. How many complex rearrangements happened? (Split+merge event) N/A\n",
    "4. How many NPB TADs remained conserved in Oruf (% conservation >=50) (Use info from Method 3, cov>=50%) 391"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split event\n",
    "1 npb_tad = 2+ az_tads AND sum(Tlengths/Qlengths from last col)>=50\n",
    "#actually if sum>= 0 the results is the same\n",
    "merge event\n",
    "2+ npb_tad = 1 az_tad AND sum(Tlengths/Qlengths)>=50\n",
    "#actually if sum>= 0 the results is the same\n",
    "conserved\n",
    "1 npb_tad = 1 az_tad AND sum(Tlengths/Qlengths)>=50\n",
    "#took from the Method 3 results\n",
    "complex event\n",
    "how to define???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
